{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "HW_2.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8jVfEGy7zJj"
      },
      "source": [
        "### Импорт"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1doeDq6oOdE",
        "outputId": "9ed027f4-f9bf-4e84-e4ba-a414c6a67c7f"
      },
      "source": [
        "pip install torchmetrics"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.6.0-py3-none-any.whl (329 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 19.5 MB/s eta 0:00:01\r\u001b[K     |██                              | 20 kB 23.0 MB/s eta 0:00:01\r\u001b[K     |███                             | 30 kB 21.7 MB/s eta 0:00:01\r\u001b[K     |████                            | 40 kB 18.1 MB/s eta 0:00:01\r\u001b[K     |█████                           | 51 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |██████                          | 61 kB 13.2 MB/s eta 0:00:01\r\u001b[K     |███████                         | 71 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |████████                        | 81 kB 13.3 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 92 kB 14.2 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 102 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 112 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 122 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 133 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 143 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 153 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 163 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 174 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 184 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 194 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 204 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 215 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 225 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 235 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 245 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 256 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 266 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 276 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 286 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 296 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 307 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 317 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 327 kB 11.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 329 kB 11.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.19.5)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.1->torchmetrics) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.6)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 849
        },
        "id": "5OIh8ez-oQJ8",
        "outputId": "610c0d01-9d4a-4c8a-a38c-e36395e75d23"
      },
      "source": [
        "pip install ipdb"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ipdb\n",
            "  Downloading ipdb-0.13.9.tar.gz (16 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from ipdb) (57.4.0)\n",
            "Collecting ipython>=7.17.0\n",
            "  Downloading ipython-7.29.0-py3-none-any.whl (790 kB)\n",
            "\u001b[K     |████████████████████████████████| 790 kB 13.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: toml>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from ipdb) (0.10.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipdb) (4.4.2)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.2.0)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.22-py3-none-any.whl (374 kB)\n",
            "\u001b[K     |████████████████████████████████| 374 kB 49.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (5.1.1)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.1.3)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.18.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (2.6.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.17.0->ipdb) (0.8.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.17.0->ipdb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.17.0->ipdb) (0.2.5)\n",
            "Building wheels for collected packages: ipdb\n",
            "  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipdb: filename=ipdb-0.13.9-py3-none-any.whl size=11648 sha256=953e024e4be0a7e11eab8e80f04d241494014cedebc842ace808d0a2e069816d\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/cd/cc/aaf92acae337a28fdd2aa4d632196a59745c8c39f76eaeed01\n",
            "Successfully built ipdb\n",
            "Installing collected packages: prompt-toolkit, ipython, ipdb\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.22 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.29.0 which is incompatible.\u001b[0m\n",
            "Successfully installed ipdb-0.13.9 ipython-7.29.0 prompt-toolkit-3.0.22\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0VYCjlL7w0D"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.optim as optim\n",
        "from torchmetrics import F1\n",
        "from torchmetrics.functional import f1, recall\n",
        "import ipdb\n",
        "\n",
        "import gensim"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4KtavV77GKI"
      },
      "source": [
        "### Готовим данные\n",
        "https://github.com/daria-sa/NNmethods_ba_hse21-22/blob/main/7_mlp_torch.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jukR35ap7Lyd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be8701ea-a116-443a-9dc8-14f29c03b200"
      },
      "source": [
        "!wget https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv\n",
        "!wget https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-26 17:27:09--  https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.67.18, 2620:100:6023:18::a27d:4312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.67.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/r6u59ljhhjdg6j0/negative.csv [following]\n",
            "--2021-11-26 17:27:09--  https://www.dropbox.com/s/raw/r6u59ljhhjdg6j0/negative.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucdeebf682acc8871ff24cdbadd1.dl.dropboxusercontent.com/cd/0/inline/Bat9kgEtQeuxgtP6B8WE69qG1O5V9-6io46Zjc42dBnTMticqZnp_BEajO3AOjraYtOhoGWknHxec_ollG8ewlab73xoZDlC_ouhQ2-hCAiw81xdttpfQduPxpul6PDrECj96pOTRQk4ecxWM_ub4UCO/file# [following]\n",
            "--2021-11-26 17:27:10--  https://ucdeebf682acc8871ff24cdbadd1.dl.dropboxusercontent.com/cd/0/inline/Bat9kgEtQeuxgtP6B8WE69qG1O5V9-6io46Zjc42dBnTMticqZnp_BEajO3AOjraYtOhoGWknHxec_ollG8ewlab73xoZDlC_ouhQ2-hCAiw81xdttpfQduPxpul6PDrECj96pOTRQk4ecxWM_ub4UCO/file\n",
            "Resolving ucdeebf682acc8871ff24cdbadd1.dl.dropboxusercontent.com (ucdeebf682acc8871ff24cdbadd1.dl.dropboxusercontent.com)... 162.125.68.15, 2620:100:6023:15::a27d:430f\n",
            "Connecting to ucdeebf682acc8871ff24cdbadd1.dl.dropboxusercontent.com (ucdeebf682acc8871ff24cdbadd1.dl.dropboxusercontent.com)|162.125.68.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24450101 (23M) [text/plain]\n",
            "Saving to: ‘negative.csv’\n",
            "\n",
            "negative.csv        100%[===================>]  23.32M  16.3MB/s    in 1.4s    \n",
            "\n",
            "2021-11-26 17:27:12 (16.3 MB/s) - ‘negative.csv’ saved [24450101/24450101]\n",
            "\n",
            "--2021-11-26 17:27:12--  https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.67.18, 2620:100:6023:18::a27d:4312\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.67.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/fnpq3z4bcnoktiv/positive.csv [following]\n",
            "--2021-11-26 17:27:12--  https://www.dropbox.com/s/raw/fnpq3z4bcnoktiv/positive.csv\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc87655ab075ab8605e0b6360bd6.dl.dropboxusercontent.com/cd/0/inline/BavXUD3zgxFvNLIIHLgf04luDGitRf2MWvt5PqO5RD-MSKswTJHE09C_75X3GkNsToLcgEhPVjDj3UhDflPhyf5jdbdN8TjgZCoN5yc2qlQKbmSsh020eOodcsNKyQAg2Zcp8NKtCOnLgVlQGncCSaNg/file# [following]\n",
            "--2021-11-26 17:27:12--  https://uc87655ab075ab8605e0b6360bd6.dl.dropboxusercontent.com/cd/0/inline/BavXUD3zgxFvNLIIHLgf04luDGitRf2MWvt5PqO5RD-MSKswTJHE09C_75X3GkNsToLcgEhPVjDj3UhDflPhyf5jdbdN8TjgZCoN5yc2qlQKbmSsh020eOodcsNKyQAg2Zcp8NKtCOnLgVlQGncCSaNg/file\n",
            "Resolving uc87655ab075ab8605e0b6360bd6.dl.dropboxusercontent.com (uc87655ab075ab8605e0b6360bd6.dl.dropboxusercontent.com)... 162.125.68.15, 2620:100:6023:15::a27d:430f\n",
            "Connecting to uc87655ab075ab8605e0b6360bd6.dl.dropboxusercontent.com (uc87655ab075ab8605e0b6360bd6.dl.dropboxusercontent.com)|162.125.68.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 26233379 (25M) [text/plain]\n",
            "Saving to: ‘positive.csv’\n",
            "\n",
            "positive.csv        100%[===================>]  25.02M  11.1MB/s    in 2.3s    \n",
            "\n",
            "2021-11-26 17:27:15 (11.1 MB/s) - ‘positive.csv’ saved [26233379/26233379]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3ZYzgRS7GKJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b7e8b3d-643b-4ed5-fa0c-6f1db33110f4"
      },
      "source": [
        "pos_tweets = pd.read_csv('positive.csv', encoding='utf-8', sep=';', header=None,  names=[0,1,2,'text','tone',5,6,7,8,9,10,11])\n",
        "pos_tweets['tone'] = 1\n",
        "neg_tweets = pd.read_csv('negative.csv', encoding='utf-8', sep=';', header=None, names=[0,1,2,'text','tone',5,6,7,8,9,10,11])\n",
        "neg_tweets['tone'] = 0\n",
        "all_tweets_data = pos_tweets.append(neg_tweets)\n",
        "print(len(all_tweets_data))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "226834\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsQnGv4k7GKK"
      },
      "source": [
        "tweets_data = shuffle(all_tweets_data[['text','tone']])\n",
        "train_sentences, val_sentences = train_test_split(tweets_data, test_size=0.1)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mg9zZOHD7GKL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25477fe2-aa71-4b30-a0b0-9e222588ac8f"
      },
      "source": [
        "def preprocess(text):\n",
        "    tokens = text.lower().split()\n",
        "    tokens = [token.strip(punctuation) for token in tokens]\n",
        "    return tokens\n",
        "vocab = Counter()\n",
        "\n",
        "for text in tweets_data['text']:\n",
        "    vocab.update(preprocess(text))\n",
        "print('всего уникальных токенов:', len(vocab))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "всего уникальных токенов: 366250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GUHSUXz7GKM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a6d2373-d5e9-407f-f0f4-4417c61ef67d"
      },
      "source": [
        "filtered_vocab = set()\n",
        "\n",
        "for word in vocab:\n",
        "    if vocab[word] > 2:\n",
        "        filtered_vocab.add(word)\n",
        "print('уникальных токенов, втретившихся больше 2 раз:', len(filtered_vocab))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "уникальных токенов, втретившихся больше 2 раз: 64423\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hvB2ouS7GKM"
      },
      "source": [
        "#создаем словарь с индексами word2id, для спецсимвола паддинга дефолтный индекс - 0\n",
        "word2id = {'PAD':0}\n",
        "\n",
        "for word in filtered_vocab:\n",
        "    word2id[word] = len(word2id)\n",
        "#обратный словарь для того, чтобы раскодировать последовательность\n",
        "id2word = {i:word for word, i in word2id.items()}"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l32ttjqK7GKN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed77da57-341b-4041-c2a2-6aa776cfeb3e"
      },
      "source": [
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "DEVICE"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_vnUoi27GKN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96237cd8-531c-4f43-f146-ef894bc453a7"
      },
      "source": [
        "MAX_LEN = 0\n",
        "\n",
        "for text in tweets_data.text:\n",
        "    tokens = preprocess(text)\n",
        "    MAX_LEN = max(len(tokens), MAX_LEN)\n",
        "MAX_LEN"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR6azNUp7GKO"
      },
      "source": [
        "X = torch.LongTensor(size=(train_sentences.shape[0], MAX_LEN))\n",
        "\n",
        "for i, text in enumerate(train_sentences.text):\n",
        "    tokens = preprocess(text) # токенизируем\n",
        "    \n",
        "    ids = [word2id[token] for token in tokens if token in word2id][:MAX_LEN]\n",
        "\n",
        "    ids = F.pad(torch.LongTensor(ids), (0,MAX_LEN-len(ids)))\n",
        "    X[i] = ids"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teCQytU77GKO"
      },
      "source": [
        "class TweetsDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataset, word2id, DEVICE):\n",
        "        self.dataset = dataset['text'].values\n",
        "        self.word2id = word2id\n",
        "        self.length = dataset.shape[0]\n",
        "        self.target = dataset['tone'].values\n",
        "        self.device = DEVICE\n",
        "\n",
        "    def __len__(self): #это обязательный метод, он должен уметь считать длину датасета\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index): #еще один обязательный метод. По индексу возвращает элемент выборки\n",
        "        tokens = self.preprocess(self.dataset[index]) # токенизируем\n",
        "        ids = torch.LongTensor([self.word2id[token] for token in tokens if token in self.word2id])\n",
        "        y = [self.target[index]]\n",
        "        return ids, y\n",
        "    \n",
        "    def preprocess(self, text):\n",
        "        tokens = text.lower().split()\n",
        "        tokens = [token.strip(punctuation) for token in tokens]\n",
        "        tokens = [token for token in tokens if token]\n",
        "        return tokens\n",
        "\n",
        "    def collate_fn(self, batch): #этот метод можно реализовывать и отдельно,\n",
        "        # он понадобится для DataLoader во время итерации по батчам\n",
        "        ids, y = list(zip(*batch))\n",
        "        padded_ids = pad_sequence(ids, batch_first=True).to(self.device)\n",
        "        #мы хотим применять BCELoss, он будет брать на вход predicted размера batch_size x 1 (так как для каждого семпла модель будет отдавать одно число), target размера batch_size x 1\n",
        "        y = torch.Tensor(y).to(self.device) # tuple ([1], [0], [1])  -> Tensor [[1.], [0.], [1.]] \n",
        "        return padded_ids, y"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHIb6Mey7GKP"
      },
      "source": [
        "train_dataset = TweetsDataset(train_sentences, word2id, DEVICE)\n",
        "train_sampler = SequentialSampler(train_dataset)\n",
        "train_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn, sampler=train_sampler, batch_size=1024)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TE2jmWle7GKP"
      },
      "source": [
        "val_dataset = TweetsDataset(val_sentences, word2id, DEVICE)\n",
        "val_sampler = SequentialSampler(val_dataset)\n",
        "val_iterator = DataLoader(val_dataset, collate_fn = val_dataset.collate_fn, sampler=val_sampler, batch_size=1024)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTREoBTs7GKQ"
      },
      "source": [
        "### Функции обучения и валидации\n",
        "https://github.com/daria-sa/NNmethods_ba_hse21-22/blob/main/09_CNN.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbunNJny7GKQ"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0 # для подсчета среднего лосса на всех батчах\n",
        "\n",
        "    model.train()  # ставим модель в обучение, явно указываем, что сейчас надо будет хранить градиенты у всех весов\n",
        "\n",
        "    for i, (texts, ys) in enumerate(iterator): #итерируемся по батчам\n",
        "        optimizer.zero_grad()  #обнуляем градиенты\n",
        "        preds = model(texts)  #прогоняем данные через модель\n",
        "        loss = criterion(preds, ys) #считаем значение функции потерь  \n",
        "        loss.backward() #считаем градиенты  \n",
        "        optimizer.step() #обновляем веса \n",
        "        epoch_loss += loss.item() #сохраняем значение функции потерь\n",
        "        if not (i + 1) % int(len(iterator)/5):\n",
        "            print(f'Train loss: {epoch_loss/i}')      \n",
        "    return  epoch_loss / len(iterator) # возвращаем среднее значение лосса по всей выборке\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_metric = 0\n",
        "    model.eval() \n",
        "    with torch.no_grad():\n",
        "        for i, (texts, ys) in enumerate(iterator):   \n",
        "            preds = model(texts)  # делаем предсказания на тесте\n",
        "            loss = criterion(preds, ys)   # считаем значения функции ошибки для статистики  \n",
        "            epoch_loss += loss.item()\n",
        "            batch_metric = f1(preds.round().long(), ys.long(), ignore_index=0)\n",
        "            epoch_metric += batch_metric\n",
        "\n",
        "            if not (i + 1) % int(len(iterator)/5):\n",
        "                print(f'Val loss: {epoch_loss/i}, Val f1: {epoch_metric/i}')\n",
        "        \n",
        "    return epoch_metric / len(iterator), epoch_loss / len(iterator) # возвращаем среднее значение по всей выборке"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9mZTqTS7GKQ"
      },
      "source": [
        "### Архитектура 1, обучаемые эмбеддинги"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-A6_-ON7GKR"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.bigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=2, padding='same')\n",
        "        self.trigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=80, kernel_size=3, padding='same')\n",
        "        self.bigrams_sec = nn.Conv1d(in_channels=180, out_channels=100, kernel_size=2, padding='same')\n",
        "        self.hidden = nn.Linear(in_features=100, out_features=1)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        embedded = embedded.transpose(1,2)\n",
        "        feature_map_bigrams = self.bigrams(embedded)\n",
        "        feature_map_trigrams = self.trigrams(embedded)\n",
        "        concat = torch.cat((feature_map_bigrams, feature_map_trigrams), 1)\n",
        "        feature_map_bigrams_sec = self.bigrams_sec(concat)\n",
        "        pooling = feature_map_bigrams_sec.max(2)[0]\n",
        "        logits = self.hidden(pooling) \n",
        "        logits = self.out(logits)      \n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNMn42667GKR"
      },
      "source": [
        "model = CNN(len(word2id), 100)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "criterion = nn.BCELoss() \n",
        "\n",
        "# веса модели и значения лосса храним там же, где и все остальные тензоры\n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CaWLXF4r7GKR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db5a6484-e8b2-45bd-b956-87515ece9c79"
      },
      "source": [
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(20):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = train(model, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = evaluate(model, train_iterator, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py:298: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  ../aten/src/ATen/native/Convolution.cpp:647.)\n",
            "  self.padding, self.dilation, self.groups)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.6847423131649311\n",
            "Train loss: 0.6571321064912821\n",
            "Train loss: 0.6436778872954745\n",
            "Train loss: 0.6339144031956511\n",
            "Train loss: 0.6264340053850682\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5983882149060568, Val f1: 0.6943443417549133\n",
            "Val loss: 0.589506849457946, Val f1: 0.6886432766914368\n",
            "Val loss: 0.5863617498333714, Val f1: 0.6866886019706726\n",
            "Val loss: 0.583603135819705, Val f1: 0.6874958872795105\n",
            "Val loss: 0.5820002337196963, Val f1: 0.6876813173294067\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.7918918132781982, Val f1: 0.8887373805046082\n",
            "Val loss: 0.6802528074809483, Val f1: 0.7552644610404968\n",
            "Val loss: 0.6461874571713534, Val f1: 0.721592128276825\n",
            "Val loss: 0.6311427672704061, Val f1: 0.7099669575691223\n",
            "Val loss: 0.6208034722428573, Val f1: 0.704522967338562\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 0.595395945585691\n",
            "Train loss: 0.5824286899989164\n",
            "Train loss: 0.5756451347294975\n",
            "Train loss: 0.5697135055589976\n",
            "Train loss: 0.5653553416381529\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5485095656835116, Val f1: 0.7351173162460327\n",
            "Val loss: 0.5402046204367771, Val f1: 0.72905433177948\n",
            "Val loss: 0.5371562445364079, Val f1: 0.7272793054580688\n",
            "Val loss: 0.5336257166832498, Val f1: 0.7288960814476013\n",
            "Val loss: 0.5314341254869298, Val f1: 0.7297965288162231\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.7528127233187357, Val f1: 0.9071029424667358\n",
            "Val loss: 0.6438450813293457, Val f1: 0.7807422280311584\n",
            "Val loss: 0.6108267144723372, Val f1: 0.7511014938354492\n",
            "Val loss: 0.5965235988299052, Val f1: 0.7362877726554871\n",
            "Val loss: 0.5858652748559651, Val f1: 0.7316403985023499\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.5464903880388309\n",
            "Train loss: 0.5356351601172097\n",
            "Train loss: 0.5306929863300645\n",
            "Train loss: 0.5258646603650267\n",
            "Train loss: 0.5225544431101736\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5089283119409512, Val f1: 0.766822338104248\n",
            "Val loss: 0.5010290990901899, Val f1: 0.760162353515625\n",
            "Val loss: 0.49788552497615335, Val f1: 0.7587040066719055\n",
            "Val loss: 0.4936567580549972, Val f1: 0.7605668902397156\n",
            "Val loss: 0.49086528821806213, Val f1: 0.7618989944458008\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.7300441463788351, Val f1: 0.9375442266464233\n",
            "Val loss: 0.6226924061775208, Val f1: 0.8054085373878479\n",
            "Val loss: 0.5909366770224138, Val f1: 0.7724597454071045\n",
            "Val loss: 0.5770895640055339, Val f1: 0.7576801180839539\n",
            "Val loss: 0.5664219385699222, Val f1: 0.7513235807418823\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.5078186797790039\n",
            "Train loss: 0.4980866211124613\n",
            "Train loss: 0.49396918576304655\n",
            "Train loss: 0.4897006761352971\n",
            "Train loss: 0.48682281584595916\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4745077681847108, Val f1: 0.7923123240470886\n",
            "Val loss: 0.4669790177405635, Val f1: 0.7850440144538879\n",
            "Val loss: 0.4636983170228846, Val f1: 0.7835112810134888\n",
            "Val loss: 0.4590888222058614, Val f1: 0.7849807739257812\n",
            "Val loss: 0.45579399056171055, Val f1: 0.7867116332054138\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.7173082232475281, Val f1: 0.9547251462936401\n",
            "Val loss: 0.6105338420186724, Val f1: 0.8203279972076416\n",
            "Val loss: 0.5792903954332526, Val f1: 0.7877612709999084\n",
            "Val loss: 0.5656771103541056, Val f1: 0.7689908742904663\n",
            "Val loss: 0.5551934869665849, Val f1: 0.762006938457489\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.4738326431849064\n",
            "Train loss: 0.46484255073945735\n",
            "Train loss: 0.4611053792368464\n",
            "Train loss: 0.45716779861810075\n",
            "Train loss: 0.4545363661032825\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.44393250269767565, Val f1: 0.8136577010154724\n",
            "Val loss: 0.43682234347621096, Val f1: 0.8055282831192017\n",
            "Val loss: 0.4333565245155527, Val f1: 0.8040156364440918\n",
            "Val loss: 0.428447702395841, Val f1: 0.8057053685188293\n",
            "Val loss: 0.4246357325932488, Val f1: 0.8078474402427673\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.7111384073893229, Val f1: 0.9694247245788574\n",
            "Val loss: 0.6041542717388698, Val f1: 0.828278124332428\n",
            "Val loss: 0.5739210437644612, Val f1: 0.7947980761528015\n",
            "Val loss: 0.5605480909347534, Val f1: 0.7764596939086914\n",
            "Val loss: 0.5501822076345745, Val f1: 0.7689085006713867\n",
            "\n",
            "starting Epoch 5\n",
            "Training...\n",
            "Train loss: 0.4427976294969901\n",
            "Train loss: 0.4342261496223981\n",
            "Train loss: 0.43078032561710905\n",
            "Train loss: 0.4271177239013168\n",
            "Train loss: 0.4246291498742511\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.41520749987700045, Val f1: 0.831669270992279\n",
            "Val loss: 0.40874362821820415, Val f1: 0.8231139779090881\n",
            "Val loss: 0.4051493223474807, Val f1: 0.8213757872581482\n",
            "Val loss: 0.40003223801558874, Val f1: 0.8229694366455078\n",
            "Val loss: 0.39577831679852166, Val f1: 0.8255527019500732\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.7112672726313273, Val f1: 0.9693254828453064\n",
            "Val loss: 0.6034133008548191, Val f1: 0.8292262554168701\n",
            "Val loss: 0.5740071643482555, Val f1: 0.7976154088973999\n",
            "Val loss: 0.5607787688573201, Val f1: 0.7797269225120544\n",
            "Val loss: 0.5503436468149486, Val f1: 0.7720016837120056\n",
            "\n",
            "starting Epoch 6\n",
            "Training...\n",
            "Train loss: 0.41337467921085846\n",
            "Train loss: 0.4053158156479461\n",
            "Train loss: 0.4021023201341389\n",
            "Train loss: 0.3986109442680887\n",
            "Train loss: 0.39630768856211523\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.38768566571749175, Val f1: 0.8489927649497986\n",
            "Val loss: 0.38187293085870866, Val f1: 0.8399096131324768\n",
            "Val loss: 0.37814375257291716, Val f1: 0.8378480076789856\n",
            "Val loss: 0.3729485552640831, Val f1: 0.8396337628364563\n",
            "Val loss: 0.3683574744804421, Val f1: 0.8420386910438538\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.71702508131663, Val f1: 0.9675024747848511\n",
            "Val loss: 0.6075903517859322, Val f1: 0.8325744867324829\n",
            "Val loss: 0.5783715735782277, Val f1: 0.7989861369132996\n",
            "Val loss: 0.5651868383089701, Val f1: 0.7798956036567688\n",
            "Val loss: 0.5546991354540775, Val f1: 0.7721637487411499\n",
            "\n",
            "starting Epoch 7\n",
            "Training...\n",
            "Train loss: 0.38516115530943257\n",
            "Train loss: 0.37775648734237577\n",
            "Train loss: 0.3746639505153945\n",
            "Train loss: 0.3713626312384815\n",
            "Train loss: 0.36916873001273554\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3616051964270763, Val f1: 0.8642696738243103\n",
            "Val loss: 0.35656668943694875, Val f1: 0.8546571731567383\n",
            "Val loss: 0.352753290108272, Val f1: 0.8525039553642273\n",
            "Val loss: 0.3474017321688574, Val f1: 0.8541238307952881\n",
            "Val loss: 0.34247721469582026, Val f1: 0.8567913770675659\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.7284120321273804, Val f1: 0.9750024676322937\n",
            "Val loss: 0.6159752692495074, Val f1: 0.8381816148757935\n",
            "Val loss: 0.5867463187737898, Val f1: 0.8031453490257263\n",
            "Val loss: 0.5730305314064026, Val f1: 0.7830906510353088\n",
            "Val loss: 0.5625794247577065, Val f1: 0.7744478583335876\n",
            "\n",
            "starting Epoch 8\n",
            "Training...\n",
            "Train loss: 0.35814219407546216\n",
            "Train loss: 0.3512886758846573\n",
            "Train loss: 0.3482247383153739\n",
            "Train loss: 0.3451293283288584\n",
            "Train loss: 0.3430379020359049\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3363513541527284, Val f1: 0.8784586787223816\n",
            "Val loss: 0.332058957483195, Val f1: 0.8692614436149597\n",
            "Val loss: 0.3282682407303017, Val f1: 0.8668060898780823\n",
            "Val loss: 0.32289790788536554, Val f1: 0.8681455254554749\n",
            "Val loss: 0.3176661663169238, Val f1: 0.8708627223968506\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.7432225942611694, Val f1: 0.9687172770500183\n",
            "Val loss: 0.6273956639426095, Val f1: 0.8368211984634399\n",
            "Val loss: 0.5985279787670482, Val f1: 0.8008249402046204\n",
            "Val loss: 0.5842453837394714, Val f1: 0.781927227973938\n",
            "Val loss: 0.5737281112294448, Val f1: 0.7737502455711365\n",
            "\n",
            "starting Epoch 9\n",
            "Training...\n",
            "Train loss: 0.33220521074074966\n",
            "Train loss: 0.3258305474927154\n",
            "Train loss: 0.32285245276298846\n",
            "Train loss: 0.3198886082607245\n",
            "Train loss: 0.31797606658995453\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3117287258307139, Val f1: 0.8922973275184631\n",
            "Val loss: 0.30807217314273494, Val f1: 0.8822256326675415\n",
            "Val loss: 0.3042632904874177, Val f1: 0.8800473213195801\n",
            "Val loss: 0.29895985951213716, Val f1: 0.8813849687576294\n",
            "Val loss: 0.2935378533511905, Val f1: 0.8839659690856934\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.7623435457547506, Val f1: 0.9702010154724121\n",
            "Val loss: 0.642449540751321, Val f1: 0.8384365439414978\n",
            "Val loss: 0.6134036941961809, Val f1: 0.8024518489837646\n",
            "Val loss: 0.5983139673868815, Val f1: 0.7836662530899048\n",
            "Val loss: 0.5876192293669048, Val f1: 0.7752681970596313\n",
            "\n",
            "starting Epoch 10\n",
            "Training...\n",
            "Train loss: 0.30733585128417384\n",
            "Train loss: 0.3015431999405728\n",
            "Train loss: 0.29850788151516633\n",
            "Train loss: 0.29571727874144066\n",
            "Train loss: 0.2939205109773569\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.2884500164252061, Val f1: 0.905637800693512\n",
            "Val loss: 0.2855285719225678, Val f1: 0.8943748474121094\n",
            "Val loss: 0.2816733890721778, Val f1: 0.8925005197525024\n",
            "Val loss: 0.2763485201874619, Val f1: 0.8938584923744202\n",
            "Val loss: 0.27078452159711464, Val f1: 0.8964269161224365\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.7854214708010355, Val f1: 0.9677242040634155\n",
            "Val loss: 0.6613690171922956, Val f1: 0.8362133502960205\n",
            "Val loss: 0.6316110979426991, Val f1: 0.8008330464363098\n",
            "Val loss: 0.6156303763389588, Val f1: 0.7820379734039307\n",
            "Val loss: 0.604774926838122, Val f1: 0.7738626003265381\n",
            "\n",
            "starting Epoch 11\n",
            "Training...\n",
            "Train loss: 0.2834864472731566\n",
            "Train loss: 0.2781749552563776\n",
            "Train loss: 0.2752005148585103\n",
            "Train loss: 0.2724970630692236\n",
            "Train loss: 0.270863743358521\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.2656756265041156, Val f1: 0.9177354574203491\n",
            "Val loss: 0.26337661980828153, Val f1: 0.9055672287940979\n",
            "Val loss: 0.2595609512900104, Val f1: 0.9040024280548096\n",
            "Val loss: 0.2542942518708091, Val f1: 0.9051059484481812\n",
            "Val loss: 0.24868382217746285, Val f1: 0.9078095555305481\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.8120928804079691, Val f1: 0.9684020280838013\n",
            "Val loss: 0.6823124204363141, Val f1: 0.8370411992073059\n",
            "Val loss: 0.6518658833070234, Val f1: 0.8011417388916016\n",
            "Val loss: 0.634930940469106, Val f1: 0.7831122875213623\n",
            "Val loss: 0.6240455727828177, Val f1: 0.7745071053504944\n",
            "\n",
            "starting Epoch 12\n",
            "Training...\n",
            "Train loss: 0.26076036386000806\n",
            "Train loss: 0.2559332376039481\n",
            "Train loss: 0.252943197969629\n",
            "Train loss: 0.2504136192161332\n",
            "Train loss: 0.24897333421749085\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.24387198457351097, Val f1: 0.9301542639732361\n",
            "Val loss: 0.24195702638052688, Val f1: 0.9179911613464355\n",
            "Val loss: 0.2382184689034935, Val f1: 0.9156255722045898\n",
            "Val loss: 0.23288221966545536, Val f1: 0.9165991544723511\n",
            "Val loss: 0.22730200958611377, Val f1: 0.9191151261329651\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.8401062885920206, Val f1: 0.9696227312088013\n",
            "Val loss: 0.7050299644470215, Val f1: 0.8391644954681396\n",
            "Val loss: 0.6735019846396013, Val f1: 0.8023740649223328\n",
            "Val loss: 0.6557275493939717, Val f1: 0.7850920557975769\n",
            "Val loss: 0.6445063227101376, Val f1: 0.7762637734413147\n",
            "\n",
            "starting Epoch 13\n",
            "Training...\n",
            "Train loss: 0.23916686383577493\n",
            "Train loss: 0.23470531253120566\n",
            "Train loss: 0.23182270193801208\n",
            "Train loss: 0.22952176986625358\n",
            "Train loss: 0.22825082522540835\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.22339642812044191, Val f1: 0.9414255023002625\n",
            "Val loss: 0.22198097483266757, Val f1: 0.9286450147628784\n",
            "Val loss: 0.2183746189630332, Val f1: 0.9261893033981323\n",
            "Val loss: 0.21300604021024405, Val f1: 0.9272487759590149\n",
            "Val loss: 0.20749643834392031, Val f1: 0.9294982552528381\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.8705402215321859, Val f1: 0.96923828125\n",
            "Val loss: 0.7302830559866769, Val f1: 0.8410980105400085\n",
            "Val loss: 0.6975768154317682, Val f1: 0.8043000102043152\n",
            "Val loss: 0.6789860129356384, Val f1: 0.7873988747596741\n",
            "Val loss: 0.667251097528558, Val f1: 0.7782585024833679\n",
            "\n",
            "starting Epoch 14\n",
            "Training...\n",
            "Train loss: 0.21877349722079742\n",
            "Train loss: 0.21476746095886715\n",
            "Train loss: 0.21198669201185724\n",
            "Train loss: 0.20993298432737026\n",
            "Train loss: 0.20885568482791958\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.2062879456923558, Val f1: 0.9507971405982971\n",
            "Val loss: 0.20519867445094675, Val f1: 0.9373611211776733\n",
            "Val loss: 0.2016396858111149, Val f1: 0.9345481991767883\n",
            "Val loss: 0.1960670676246379, Val f1: 0.9354475140571594\n",
            "Val loss: 0.19069818953922646, Val f1: 0.9375307559967041\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9060231844584147, Val f1: 0.9743202924728394\n",
            "Val loss: 0.7605206285204206, Val f1: 0.8442000150680542\n",
            "Val loss: 0.7258254452185198, Val f1: 0.8081870079040527\n",
            "Val loss: 0.7060301184654236, Val f1: 0.7917215824127197\n",
            "Val loss: 0.693572903934278, Val f1: 0.7823730111122131\n",
            "\n",
            "starting Epoch 15\n",
            "Training...\n",
            "Train loss: 0.2000284099426025\n",
            "Train loss: 0.1964199237431152\n",
            "Train loss: 0.19359265330458889\n",
            "Train loss: 0.1917353278061129\n",
            "Train loss: 0.19073393091035248\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.1922056285234598, Val f1: 0.9573221802711487\n",
            "Val loss: 0.19129731062846847, Val f1: 0.9439254999160767\n",
            "Val loss: 0.18793772461534547, Val f1: 0.9410142302513123\n",
            "Val loss: 0.1822185330795792, Val f1: 0.9420475959777832\n",
            "Val loss: 0.17694094762131196, Val f1: 0.9437515139579773\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9445488651593527, Val f1: 0.9799076914787292\n",
            "Val loss: 0.7938370704650879, Val f1: 0.8471200466156006\n",
            "Val loss: 0.7573870095339689, Val f1: 0.8115584850311279\n",
            "Val loss: 0.7368921955426534, Val f1: 0.7941181659698486\n",
            "Val loss: 0.7233338606984991, Val f1: 0.785150945186615\n",
            "\n",
            "starting Epoch 16\n",
            "Training...\n",
            "Train loss: 0.18333591062289017\n",
            "Train loss: 0.1795555558762973\n",
            "Train loss: 0.17690760473243328\n",
            "Train loss: 0.17522567669925448\n",
            "Train loss: 0.17409820961368144\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.17249066822039774, Val f1: 0.9644561409950256\n",
            "Val loss: 0.17198014466822903, Val f1: 0.9518055319786072\n",
            "Val loss: 0.1688984460940882, Val f1: 0.948918879032135\n",
            "Val loss: 0.16355678457883918, Val f1: 0.9500243663787842\n",
            "Val loss: 0.15840360591534394, Val f1: 0.9518568515777588\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9779924154281616, Val f1: 0.9750524759292603\n",
            "Val loss: 0.820963340146201, Val f1: 0.8438078165054321\n",
            "Val loss: 0.7836772203445435, Val f1: 0.8093471527099609\n",
            "Val loss: 0.7623559395472209, Val f1: 0.7913910150527954\n",
            "Val loss: 0.7484099990443179, Val f1: 0.7820290923118591\n",
            "\n",
            "starting Epoch 17\n",
            "Training...\n",
            "Train loss: 0.1673250030248593\n",
            "Train loss: 0.16344003832038445\n",
            "Train loss: 0.16126103899559052\n",
            "Train loss: 0.1598392904929395\n",
            "Train loss: 0.15842205175577695\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.1555462319117326, Val f1: 0.9717363119125366\n",
            "Val loss: 0.15524501985386957, Val f1: 0.9584927558898926\n",
            "Val loss: 0.1524153902751057, Val f1: 0.9555058479309082\n",
            "Val loss: 0.14758241940406883, Val f1: 0.9561730623245239\n",
            "Val loss: 0.14260366362468083, Val f1: 0.9578966498374939\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0154163440068562, Val f1: 0.9633006453514099\n",
            "Val loss: 0.8509279659816197, Val f1: 0.8375527858734131\n",
            "Val loss: 0.813552357933738, Val f1: 0.8025495409965515\n",
            "Val loss: 0.791154932975769, Val f1: 0.7846100926399231\n",
            "Val loss: 0.777075940056851, Val f1: 0.7752636671066284\n",
            "\n",
            "starting Epoch 18\n",
            "Training...\n",
            "Train loss: 0.15155158211023378\n",
            "Train loss: 0.14822825798882713\n",
            "Train loss: 0.14637710454584169\n",
            "Train loss: 0.14500092597877454\n",
            "Train loss: 0.14368644575752207\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.14132685921131036, Val f1: 0.9784521460533142\n",
            "Val loss: 0.14127142519890507, Val f1: 0.9647665023803711\n",
            "Val loss: 0.13866042208020427, Val f1: 0.9615824222564697\n",
            "Val loss: 0.13397658433554308, Val f1: 0.9620064496994019\n",
            "Val loss: 0.12918037988582448, Val f1: 0.9635370969772339\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0520798365275066, Val f1: 0.9635947942733765\n",
            "Val loss: 0.8827917490686689, Val f1: 0.8386372923851013\n",
            "Val loss: 0.843905811960047, Val f1: 0.8038060069084167\n",
            "Val loss: 0.8207778533299764, Val f1: 0.7856994271278381\n",
            "Val loss: 0.8061869520890085, Val f1: 0.7764506936073303\n",
            "\n",
            "starting Epoch 19\n",
            "Training...\n",
            "Train loss: 0.13688031679544693\n",
            "Train loss: 0.13463169383474544\n",
            "Train loss: 0.13276145895238683\n",
            "Train loss: 0.13103449068166925\n",
            "Train loss: 0.12993730086103156\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.12845522031570092, Val f1: 0.9836021661758423\n",
            "Val loss: 0.12862408736461325, Val f1: 0.9700648188591003\n",
            "Val loss: 0.1260461414060673, Val f1: 0.9670205116271973\n",
            "Val loss: 0.12145633231731331, Val f1: 0.9673462510108948\n",
            "Val loss: 0.11678826005839223, Val f1: 0.9686515927314758\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0925619800885518, Val f1: 0.9626336097717285\n",
            "Val loss: 0.9165184072085789, Val f1: 0.8392736911773682\n",
            "Val loss: 0.8759090792049061, Val f1: 0.8045545220375061\n",
            "Val loss: 0.8517864028612773, Val f1: 0.7864225506782532\n",
            "Val loss: 0.8362785891482705, Val f1: 0.7771397829055786\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YgebdQw7GKR"
      },
      "source": [
        "### Архитектура 1, предобученные эмбеддинги"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZK_jDMgl7GKS"
      },
      "source": [
        "texts = train_sentences.text.apply(preprocess).tolist()\n",
        "w2v = gensim.models.Word2Vec(texts, size=100, window=5, min_count=1)\n",
        "\n",
        "weights = np.zeros((len(word2id), 100))\n",
        "count = 0\n",
        "for word, i in word2id.items():\n",
        "    if word == 'PAD':\n",
        "        continue   \n",
        "    try:\n",
        "        weights[i] = w2v.wv[word]    \n",
        "    except KeyError:\n",
        "        count += 1\n",
        "        # oov словам сопоставляем случайный вектор\n",
        "        weights[i] = np.random.normal(0,0.1,100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3tMkN-V7GKS"
      },
      "source": [
        "class CNN_pretrained(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding.from_pretrained(torch.tensor(weights), freeze=True)\n",
        "        self.bigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=2, padding='same')\n",
        "        self.trigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=80, kernel_size=3, padding='same')\n",
        "        self.bigrams_sec = nn.Conv1d(in_channels=180, out_channels=100, kernel_size=2, padding='same')\n",
        "        self.hidden = nn.Linear(in_features=100, out_features=1)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        embedded = embedded.transpose(1,2)\n",
        "        feature_map_bigrams = self.bigrams(embedded)\n",
        "        feature_map_trigrams = self.trigrams(embedded)\n",
        "        concat = torch.cat((feature_map_bigrams, feature_map_trigrams), 1)\n",
        "        feature_map_bigrams_sec = self.bigrams_sec(concat)\n",
        "        pooling = feature_map_bigrams_sec.max(2)[0]\n",
        "        logits = self.hidden(pooling) \n",
        "        logits = self.out(logits)      \n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHIcPDqw7GKS"
      },
      "source": [
        "model = CNN_pretrained(len(word2id), 100)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.BCELoss() # Binary Cross Entropy\n",
        "\n",
        "# веса модели и значения лосса храним там же, где и все остальные тензоры\n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JwkdIOP7GKS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6beacbb4-93a9-4bd1-93f5-e251147c320a"
      },
      "source": [
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(20):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = train(model, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = evaluate(model, train_iterator, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 0.68256309399238\n",
            "Train loss: 0.6500966707362404\n",
            "Train loss: 0.6329495496108752\n",
            "Train loss: 0.6207773445537256\n",
            "Train loss: 0.6118407584914011\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5743533143630395, Val f1: 0.7122806310653687\n",
            "Val loss: 0.5634657846221441, Val f1: 0.7087022066116333\n",
            "Val loss: 0.5579672876526328, Val f1: 0.7101188898086548\n",
            "Val loss: 0.5534333305538826, Val f1: 0.7125400304794312\n",
            "Val loss: 0.5504873415932583, Val f1: 0.7148285508155823\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.7571129401524862, Val f1: 0.9208321571350098\n",
            "Val loss: 0.6481303232056754, Val f1: 0.7870772480964661\n",
            "Val loss: 0.618412044915286, Val f1: 0.7496301531791687\n",
            "Val loss: 0.6048306902249654, Val f1: 0.7344456315040588\n",
            "Val loss: 0.5943900315385116, Val f1: 0.7276578545570374\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 0.5709325350247897\n",
            "Train loss: 0.5543369575391842\n",
            "Train loss: 0.5449933013996157\n",
            "Train loss: 0.5377452420363636\n",
            "Train loss: 0.5330143569402359\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5119797144180689, Val f1: 0.763287365436554\n",
            "Val loss: 0.5001975920381425, Val f1: 0.7584433555603027\n",
            "Val loss: 0.4939508568338987, Val f1: 0.7591213583946228\n",
            "Val loss: 0.48775178081584425, Val f1: 0.762545645236969\n",
            "Val loss: 0.483236895284461, Val f1: 0.7659437656402588\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.7157457073529562, Val f1: 0.9554433226585388\n",
            "Val loss: 0.6107498407363892, Val f1: 0.8145605325698853\n",
            "Val loss: 0.5820601528341119, Val f1: 0.7773826122283936\n",
            "Val loss: 0.5703383803367614, Val f1: 0.7603498697280884\n",
            "Val loss: 0.5595771638970626, Val f1: 0.7548497915267944\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.5099827685417273\n",
            "Train loss: 0.4940635482721691\n",
            "Train loss: 0.4859665444418162\n",
            "Train loss: 0.4797437700835414\n",
            "Train loss: 0.47584595422648907\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4589897027382484, Val f1: 0.8016711473464966\n",
            "Val loss: 0.44769670465324496, Val f1: 0.7958653569221497\n",
            "Val loss: 0.4407711988236724, Val f1: 0.7961525917053223\n",
            "Val loss: 0.4336876003247387, Val f1: 0.799762487411499\n",
            "Val loss: 0.42782531221907344, Val f1: 0.8033863306045532\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.6978476444880167, Val f1: 0.97179114818573\n",
            "Val loss: 0.5950382266725812, Val f1: 0.8298261165618896\n",
            "Val loss: 0.5670244639570062, Val f1: 0.7905034422874451\n",
            "Val loss: 0.5577905774116516, Val f1: 0.7714343667030334\n",
            "Val loss: 0.5465866139060572, Val f1: 0.7661719918251038\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.45662455681042796\n",
            "Train loss: 0.4415760926807983\n",
            "Train loss: 0.4340824527399881\n",
            "Train loss: 0.4282874161342405\n",
            "Train loss: 0.42478660168360227\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4094913586592063, Val f1: 0.8360157608985901\n",
            "Val loss: 0.39959523836268657, Val f1: 0.8289244771003723\n",
            "Val loss: 0.3925254182154391, Val f1: 0.8287263512611389\n",
            "Val loss: 0.3845024916735835, Val f1: 0.8326756358146667\n",
            "Val loss: 0.37723030262256985, Val f1: 0.8365926742553711\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.7021518349647522, Val f1: 0.9736901521682739\n",
            "Val loss: 0.5954863684517997, Val f1: 0.8371212482452393\n",
            "Val loss: 0.5672693956982006, Val f1: 0.798764169216156\n",
            "Val loss: 0.5592854380607605, Val f1: 0.7785016894340515\n",
            "Val loss: 0.5473486404669913, Val f1: 0.7744309306144714\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.40678199667197007\n",
            "Train loss: 0.3930519204350966\n",
            "Train loss: 0.3858079982905829\n",
            "Train loss: 0.3804805115708765\n",
            "Train loss: 0.3771192258476612\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3630300339980003, Val f1: 0.8638006448745728\n",
            "Val loss: 0.3550172417978697, Val f1: 0.855814516544342\n",
            "Val loss: 0.3480004608130255, Val f1: 0.8553307056427002\n",
            "Val loss: 0.33941586512439653, Val f1: 0.8587595224380493\n",
            "Val loss: 0.33102102630102453, Val f1: 0.8633952736854553\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.7191863656044006, Val f1: 0.9763123393058777\n",
            "Val loss: 0.6073896544320243, Val f1: 0.8403385877609253\n",
            "Val loss: 0.5793930888175964, Val f1: 0.8032107949256897\n",
            "Val loss: 0.5722662488619487, Val f1: 0.7826915979385376\n",
            "Val loss: 0.5598662495613098, Val f1: 0.7781892418861389\n",
            "\n",
            "starting Epoch 5\n",
            "Training...\n",
            "Train loss: 0.35907653203377354\n",
            "Train loss: 0.34705878586708744\n",
            "Train loss: 0.34013527456451864\n",
            "Train loss: 0.3351510448275872\n",
            "Train loss: 0.33178543445452974\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3196033040682475, Val f1: 0.8899096250534058\n",
            "Val loss: 0.31361272478405433, Val f1: 0.8797842264175415\n",
            "Val loss: 0.30713329670809897, Val f1: 0.8785545825958252\n",
            "Val loss: 0.2979694142843942, Val f1: 0.8819733262062073\n",
            "Val loss: 0.28852532983724793, Val f1: 0.8871288299560547\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.7505569060643514, Val f1: 0.9797782301902771\n",
            "Val loss: 0.6317081877163478, Val f1: 0.8441330790519714\n",
            "Val loss: 0.6034556573087518, Val f1: 0.8055149912834167\n",
            "Val loss: 0.5970815539360046, Val f1: 0.784012496471405\n",
            "Val loss: 0.5839364246318215, Val f1: 0.7790761590003967\n",
            "\n",
            "starting Epoch 6\n",
            "Training...\n",
            "Train loss: 0.31417768123822337\n",
            "Train loss: 0.3033067704756049\n",
            "Train loss: 0.29681077349085766\n",
            "Train loss: 0.2920326650517542\n",
            "Train loss: 0.2885706114709078\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.27737369598486483, Val f1: 0.9126501083374023\n",
            "Val loss: 0.27366666533524475, Val f1: 0.9011462330818176\n",
            "Val loss: 0.26818525941432025, Val f1: 0.8995466828346252\n",
            "Val loss: 0.25887593992476193, Val f1: 0.9029027223587036\n",
            "Val loss: 0.24877921315892856, Val f1: 0.9079746007919312\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.7940550049146017, Val f1: 0.9853992462158203\n",
            "Val loss: 0.6654965877532959, Val f1: 0.8500324487686157\n",
            "Val loss: 0.6356356794183905, Val f1: 0.8110480308532715\n",
            "Val loss: 0.629838212331136, Val f1: 0.789233922958374\n",
            "Val loss: 0.6161789047090631, Val f1: 0.7825618386268616\n",
            "\n",
            "starting Epoch 7\n",
            "Training...\n",
            "Train loss: 0.2710962635584367\n",
            "Train loss: 0.26172461464435237\n",
            "Train loss: 0.25588780553901896\n",
            "Train loss: 0.25202088732764405\n",
            "Train loss: 0.2487492581557988\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.239217527783834, Val f1: 0.9310535788536072\n",
            "Val loss: 0.23787328449985648, Val f1: 0.9175736904144287\n",
            "Val loss: 0.23329175432690052, Val f1: 0.915422797203064\n",
            "Val loss: 0.22447078867153553, Val f1: 0.9183077812194824\n",
            "Val loss: 0.21422795449669038, Val f1: 0.9232914447784424\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.8493192394574484, Val f1: 0.9771747589111328\n",
            "Val loss: 0.7088285429137093, Val f1: 0.8461299538612366\n",
            "Val loss: 0.6768315759572116, Val f1: 0.807946503162384\n",
            "Val loss: 0.6706453839937846, Val f1: 0.7870351672172546\n",
            "Val loss: 0.6573240160942078, Val f1: 0.7790107131004333\n",
            "\n",
            "starting Epoch 8\n",
            "Training...\n",
            "Train loss: 0.23132753754273438\n",
            "Train loss: 0.22396238317972497\n",
            "Train loss: 0.21848347670390827\n",
            "Train loss: 0.21627634238896878\n",
            "Train loss: 0.21464244923998962\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.20279376552655146, Val f1: 0.9491747617721558\n",
            "Val loss: 0.20377984322324583, Val f1: 0.9348511695861816\n",
            "Val loss: 0.2003323766363769, Val f1: 0.9318947792053223\n",
            "Val loss: 0.19215771733964765, Val f1: 0.9342001080513\n",
            "Val loss: 0.18224915581282658, Val f1: 0.9386228322982788\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9084549943606058, Val f1: 0.9833081960678101\n",
            "Val loss: 0.755925144468035, Val f1: 0.8522545695304871\n",
            "Val loss: 0.7221905751661821, Val f1: 0.8127447962760925\n",
            "Val loss: 0.7160971879959106, Val f1: 0.7899585366249084\n",
            "Val loss: 0.7026324523122687, Val f1: 0.7808703780174255\n",
            "\n",
            "starting Epoch 9\n",
            "Training...\n",
            "Train loss: 0.19823153622639486\n",
            "Train loss: 0.19291912736017494\n",
            "Train loss: 0.1869218131574262\n",
            "Train loss: 0.18416623939883034\n",
            "Train loss: 0.1841596969632647\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.20572018508727735, Val f1: 0.9463563561439514\n",
            "Val loss: 0.2076181409102452, Val f1: 0.932028591632843\n",
            "Val loss: 0.20561691830639078, Val f1: 0.9284270405769348\n",
            "Val loss: 0.1960512101650238, Val f1: 0.9308311939239502\n",
            "Val loss: 0.18594499887084243, Val f1: 0.9346405863761902\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0302760402361553, Val f1: 1.0035691261291504\n",
            "Val loss: 0.8558840666498456, Val f1: 0.8662327527999878\n",
            "Val loss: 0.8133581660010598, Val f1: 0.8282583951950073\n",
            "Val loss: 0.8047488490740459, Val f1: 0.8076965808868408\n",
            "Val loss: 0.788863298140074, Val f1: 0.8006276488304138\n",
            "\n",
            "starting Epoch 10\n",
            "Training...\n",
            "Train loss: 0.1706247138671386\n",
            "Train loss: 0.16771001359329948\n",
            "Train loss: 0.16036394739351353\n",
            "Train loss: 0.15793965845925254\n",
            "Train loss: 0.15609225222663065\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.14994106594568643, Val f1: 0.9730585813522339\n",
            "Val loss: 0.15363265149578265, Val f1: 0.9569801092147827\n",
            "Val loss: 0.1519907076193505, Val f1: 0.9532908201217651\n",
            "Val loss: 0.14565624259177995, Val f1: 0.9541550874710083\n",
            "Val loss: 0.13635160150614814, Val f1: 0.9577430486679077\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.0363118648529053, Val f1: 0.965747058391571\n",
            "Val loss: 0.8534633943012783, Val f1: 0.8383923172950745\n",
            "Val loss: 0.816658540205522, Val f1: 0.799139678478241\n",
            "Val loss: 0.8106264670689901, Val f1: 0.7774097323417664\n",
            "Val loss: 0.7971094533016807, Val f1: 0.7691786289215088\n",
            "\n",
            "starting Epoch 11\n",
            "Training...\n",
            "Train loss: 0.13760118702283272\n",
            "Train loss: 0.13729676604270935\n",
            "Train loss: 0.13199211938791916\n",
            "Train loss: 0.13087361793285646\n",
            "Train loss: 0.1293018959163122\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.12653263811117563, Val f1: 0.9837403297424316\n",
            "Val loss: 0.13095378243847738, Val f1: 0.967054009437561\n",
            "Val loss: 0.13043427918137623, Val f1: 0.9629207253456116\n",
            "Val loss: 0.12412818286966228, Val f1: 0.963706910610199\n",
            "Val loss: 0.11578723413860378, Val f1: 0.9667419791221619\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1024753053983052, Val f1: 0.9847184419631958\n",
            "Val loss: 0.9086607268878392, Val f1: 0.8518475294113159\n",
            "Val loss: 0.8665083375844088, Val f1: 0.8147678375244141\n",
            "Val loss: 0.8599260648091634, Val f1: 0.7932298183441162\n",
            "Val loss: 0.8457298121954265, Val f1: 0.7848655581474304\n",
            "\n",
            "starting Epoch 12\n",
            "Training...\n",
            "Train loss: 0.11415889114141464\n",
            "Train loss: 0.11646263489994822\n",
            "Train loss: 0.1125636133952301\n",
            "Train loss: 0.11183107077325664\n",
            "Train loss: 0.11143108856063991\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.10953436104150918, Val f1: 0.9893641471862793\n",
            "Val loss: 0.11375300878588157, Val f1: 0.9731806516647339\n",
            "Val loss: 0.11286855470232603, Val f1: 0.969405472278595\n",
            "Val loss: 0.10802013295813927, Val f1: 0.9694329500198364\n",
            "Val loss: 0.10017426589026523, Val f1: 0.9721457958221436\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.1741886138916016, Val f1: 0.9579257369041443\n",
            "Val loss: 0.9679779325212751, Val f1: 0.8331827521324158\n",
            "Val loss: 0.9260382110422308, Val f1: 0.7938191294670105\n",
            "Val loss: 0.9211210370063782, Val f1: 0.7726421356201172\n",
            "Val loss: 0.9068644987909418, Val f1: 0.7647106051445007\n",
            "\n",
            "starting Epoch 13\n",
            "Training...\n",
            "Train loss: 0.09926317746822651\n",
            "Train loss: 0.10413000977869276\n",
            "Train loss: 0.10017466488756052\n",
            "Train loss: 0.09737866435129687\n",
            "Train loss: 0.09619727714838994\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.09606644434806628, Val f1: 0.9935766458511353\n",
            "Val loss: 0.09974382166998295, Val f1: 0.9784311652183533\n",
            "Val loss: 0.09901043647477607, Val f1: 0.9747138619422913\n",
            "Val loss: 0.09496987343959089, Val f1: 0.974364161491394\n",
            "Val loss: 0.08777566417939399, Val f1: 0.9765632748603821\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.2503889004389446, Val f1: 0.9552398920059204\n",
            "Val loss: 1.0277268631117684, Val f1: 0.8293535709381104\n",
            "Val loss: 0.9806845675815236, Val f1: 0.7919860482215881\n",
            "Val loss: 0.9760018587112427, Val f1: 0.7692351341247559\n",
            "Val loss: 0.9622877240180969, Val f1: 0.7601140737533569\n",
            "\n",
            "starting Epoch 14\n",
            "Training...\n",
            "Train loss: 0.08577834795682858\n",
            "Train loss: 0.09148939906419078\n",
            "Train loss: 0.08761836679167107\n",
            "Train loss: 0.0849938820461807\n",
            "Train loss: 0.0829341727304249\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.07836450072817314, Val f1: 1.0022304058074951\n",
            "Val loss: 0.08224258293645291, Val f1: 0.9864855408668518\n",
            "Val loss: 0.082144533886629, Val f1: 0.9818359017372131\n",
            "Val loss: 0.07766077091109078, Val f1: 0.9816868305206299\n",
            "Val loss: 0.07177044223788095, Val f1: 0.9830750226974487\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.306069294611613, Val f1: 0.9747159481048584\n",
            "Val loss: 1.0911132608141219, Val f1: 0.846027135848999\n",
            "Val loss: 1.0335332751274109, Val f1: 0.8115102052688599\n",
            "Val loss: 1.0241675535837809, Val f1: 0.7890765070915222\n",
            "Val loss: 1.0108523557060642, Val f1: 0.7815191149711609\n",
            "\n",
            "starting Epoch 15\n",
            "Training...\n",
            "Train loss: 0.06650785729289055\n",
            "Train loss: 0.0723377674157861\n",
            "Train loss: 0.06962776938531579\n",
            "Train loss: 0.06757172653978726\n",
            "Train loss: 0.06597228952912829\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.07605947267550689, Val f1: 1.0004581212997437\n",
            "Val loss: 0.07805063438755047, Val f1: 0.9861704707145691\n",
            "Val loss: 0.07720417278904874, Val f1: 0.982295572757721\n",
            "Val loss: 0.0747928464750074, Val f1: 0.9811938405036926\n",
            "Val loss: 0.06885907236467953, Val f1: 0.9829012751579285\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.3886302312215169, Val f1: 0.9496256709098816\n",
            "Val loss: 1.1627909796578544, Val f1: 0.8240236639976501\n",
            "Val loss: 1.1075871207497336, Val f1: 0.7849672436714172\n",
            "Val loss: 1.1009788274765016, Val f1: 0.7631288766860962\n",
            "Val loss: 1.0878566378041317, Val f1: 0.7539650797843933\n",
            "\n",
            "starting Epoch 16\n",
            "Training...\n",
            "Train loss: 0.056217034466755696\n",
            "Train loss: 0.057240954116929935\n",
            "Train loss: 0.05570052175962625\n",
            "Train loss: 0.05341543990579791\n",
            "Train loss: 0.052311034264726254\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.056402189609331965, Val f1: 1.008746862411499\n",
            "Val loss: 0.058010409527187105, Val f1: 0.995031476020813\n",
            "Val loss: 0.057117461406884076, Val f1: 0.9911694526672363\n",
            "Val loss: 0.05504831929439269, Val f1: 0.9899427890777588\n",
            "Val loss: 0.05057864784539195, Val f1: 0.9906436204910278\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.4970004955927532, Val f1: 0.9546377062797546\n",
            "Val loss: 1.2294257708958216, Val f1: 0.8296390771865845\n",
            "Val loss: 1.1651929291811856, Val f1: 0.7933191061019897\n",
            "Val loss: 1.1540474812189738, Val f1: 0.7711585164070129\n",
            "Val loss: 1.1450232392863224, Val f1: 0.7622870206832886\n",
            "\n",
            "starting Epoch 17\n",
            "Training...\n",
            "Train loss: 0.04927825698485741\n",
            "Train loss: 0.04847965502663504\n",
            "Train loss: 0.047840799259788847\n",
            "Train loss: 0.046107151223428594\n",
            "Train loss: 0.0451821657599666\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.04627025514267958, Val f1: 1.0130707025527954\n",
            "Val loss: 0.04767144313435766, Val f1: 0.9990330338478088\n",
            "Val loss: 0.04713360609940621, Val f1: 0.9947627186775208\n",
            "Val loss: 0.04451098181775906, Val f1: 0.9939176440238953\n",
            "Val loss: 0.04102540759360371, Val f1: 0.9941136837005615\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.5751784245173137, Val f1: 0.9779281616210938\n",
            "Val loss: 1.308403526033674, Val f1: 0.8482877612113953\n",
            "Val loss: 1.2464465769854458, Val f1: 0.811363697052002\n",
            "Val loss: 1.2282363096872966, Val f1: 0.7891218066215515\n",
            "Val loss: 1.216773365673266, Val f1: 0.7810006737709045\n",
            "\n",
            "starting Epoch 18\n",
            "Training...\n",
            "Train loss: 0.041280786673992105\n",
            "Train loss: 0.039498211580175385\n",
            "Train loss: 0.040169597266852354\n",
            "Train loss: 0.03999260057396484\n",
            "Train loss: 0.03995447949829263\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.03922272072388576, Val f1: 1.0148690938949585\n",
            "Val loss: 0.04050307489837272, Val f1: 1.0012788772583008\n",
            "Val loss: 0.0396090381692688, Val f1: 0.9972231984138489\n",
            "Val loss: 0.0383129811628997, Val f1: 0.9956141710281372\n",
            "Val loss: 0.03522997661657519, Val f1: 0.9956166744232178\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.6558127800623577, Val f1: 0.9586397409439087\n",
            "Val loss: 1.3720337833677019, Val f1: 0.8337980508804321\n",
            "Val loss: 1.3113347075202249, Val f1: 0.7967913746833801\n",
            "Val loss: 1.2999649922053018, Val f1: 0.774624764919281\n",
            "Val loss: 1.2859693138222945, Val f1: 0.7662839889526367\n",
            "\n",
            "starting Epoch 19\n",
            "Training...\n",
            "Train loss: 0.033725447952747345\n",
            "Train loss: 0.033254956708678715\n",
            "Train loss: 0.03331175421466347\n",
            "Train loss: 0.03464140383374391\n",
            "Train loss: 0.03557625543045339\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.08705254004169734, Val f1: 0.9890651106834412\n",
            "Val loss: 0.08741350792631318, Val f1: 0.9763134717941284\n",
            "Val loss: 0.08475459670694936, Val f1: 0.9734956622123718\n",
            "Val loss: 0.08344708788413671, Val f1: 0.9721786975860596\n",
            "Val loss: 0.07765924120930272, Val f1: 0.9738836288452148\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 1.9107491970062256, Val f1: 0.9223533868789673\n",
            "Val loss: 1.5725844587598528, Val f1: 0.7955590486526489\n",
            "Val loss: 1.5050621032714844, Val f1: 0.7542970776557922\n",
            "Val loss: 1.4906073331832885, Val f1: 0.7313272953033447\n",
            "Val loss: 1.4744669449956793, Val f1: 0.7234923839569092\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErZ5nmUX90Ma"
      },
      "source": [
        "Из лоссов видно, что в целом модель достаточно сложная, чтобы выучить закономерности в данных: она показывает хорошие результаты на трейне. Однако при валидации на тестовой выборке результаты значительно ниже, что свидетельствует о том, что модель переобучается. Например, на предобученных эмбеддингах она делает это уже после второй эпохи: лоссы на тесте начинают расти обратно. При этом модель в целом быстрее обучается на предобученных эмбеддингах, поэтому модифицировать будем эту версию."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjBNpspP_smA"
      },
      "source": [
        "### Архитектура 1, предобученные эмбеддинги + модификация"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIFIDWCKpPpV"
      },
      "source": [
        "texts = train_sentences.text.apply(preprocess).tolist()\n",
        "w2v = gensim.models.Word2Vec(texts, size=300, window=5, min_count=1)\n",
        "\n",
        "weights = np.zeros((len(word2id), 300))\n",
        "count = 0\n",
        "for word, i in word2id.items():\n",
        "    if word == 'PAD':\n",
        "        continue   \n",
        "    try:\n",
        "        weights[i] = w2v.wv[word]    \n",
        "    except KeyError:\n",
        "        count += 1\n",
        "        # oov словам сопоставляем случайный вектор\n",
        "        weights[i] = np.random.normal(0,0.1,300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAvhiny37GKT"
      },
      "source": [
        "class CNN_pretrained_mod(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding.from_pretrained(torch.tensor(weights), freeze=True)\n",
        "        self.bigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=2, padding='same')\n",
        "        self.trigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=80, kernel_size=3, padding='same')\n",
        "        self.sec = nn.Conv1d(in_channels=180, out_channels=180, kernel_size=4, padding='same')\n",
        "        self.third = nn.Conv1d(in_channels=180, out_channels=100, kernel_size=3, padding='same')\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.hidden = nn.Linear(in_features=100, out_features=1)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded = self.embedding(text)\n",
        "        embedded = embedded.transpose(1,2)\n",
        "        feature_map_bigrams = self.dropout(self.relu(self.bigrams(embedded)))\n",
        "        feature_map_trigrams = self.dropout(self.relu(self.trigrams(embedded)))\n",
        "        concat = torch.cat((feature_map_bigrams, feature_map_trigrams), 1)\n",
        "        feature_map_sec = self.sec(concat)\n",
        "        feature_map_third = self.third(feature_map_sec)\n",
        "        pooling = feature_map_third.max(2)[0]\n",
        "        logits = self.hidden(pooling) \n",
        "        logits = self.out(logits)      \n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye2cL2obAY28"
      },
      "source": [
        "model = CNN_pretrained_mod(len(word2id), 300)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.BCELoss() # Binary Cross Entropy\n",
        "\n",
        "# веса модели и значения лосса храним там же, где и все остальные тензоры\n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jtyy7kKAAcFD",
        "outputId": "c6e0a012-2ed9-48c1-aefc-5aa7a5440029"
      },
      "source": [
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(10):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = train(model, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = evaluate(model, train_iterator, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 0.683569229566134\n",
            "Train loss: 0.649019021776658\n",
            "Train loss: 0.6299428994916066\n",
            "Train loss: 0.6166419146945642\n",
            "Train loss: 0.6060836938158352\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.5484773761186844, Val f1: 0.739396870136261\n",
            "Val loss: 0.5383420126347602, Val f1: 0.7331531047821045\n",
            "Val loss: 0.531888597402252, Val f1: 0.7340630888938904\n",
            "Val loss: 0.5279921694371685, Val f1: 0.7361087799072266\n",
            "Val loss: 0.5255648886139069, Val f1: 0.7374288439750671\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.7161086996396383, Val f1: 0.9595696926116943\n",
            "Val loss: 0.62043354340962, Val f1: 0.8167509436607361\n",
            "Val loss: 0.5917400013316761, Val f1: 0.777010440826416\n",
            "Val loss: 0.5780653119087219, Val f1: 0.7594258785247803\n",
            "Val loss: 0.5709833659623799, Val f1: 0.7496312260627747\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 0.5536852601246957\n",
            "Train loss: 0.5379985171028331\n",
            "Train loss: 0.5278169137089193\n",
            "Train loss: 0.5209205146855528\n",
            "Train loss: 0.5150326809391903\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.47502200802167255, Val f1: 0.8023337125778198\n",
            "Val loss: 0.465218781293193, Val f1: 0.7946597337722778\n",
            "Val loss: 0.4573408129836331, Val f1: 0.7957180142402649\n",
            "Val loss: 0.452178290242669, Val f1: 0.7974505424499512\n",
            "Val loss: 0.448257062303361, Val f1: 0.799436628818512\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.6540624698003134, Val f1: 1.0123052597045898\n",
            "Val loss: 0.5708848578589303, Val f1: 0.8599379062652588\n",
            "Val loss: 0.5432481576095928, Val f1: 0.8197126388549805\n",
            "Val loss: 0.5326428294181824, Val f1: 0.8008158206939697\n",
            "Val loss: 0.5258790474188956, Val f1: 0.7920727133750916\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.4886158093428\n",
            "Train loss: 0.47582523996316933\n",
            "Train loss: 0.4660055922860859\n",
            "Train loss: 0.4602339640728333\n",
            "Train loss: 0.45496198295348855\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.4196473138454633, Val f1: 0.8370371460914612\n",
            "Val loss: 0.41047178755832625, Val f1: 0.8277145028114319\n",
            "Val loss: 0.4018895388150415, Val f1: 0.828704833984375\n",
            "Val loss: 0.3960008298825918, Val f1: 0.8307241201400757\n",
            "Val loss: 0.39098523969027266, Val f1: 0.8331058025360107\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.6277214388052622, Val f1: 1.0326848030090332\n",
            "Val loss: 0.5488091877528599, Val f1: 0.8775955438613892\n",
            "Val loss: 0.5223395201292905, Val f1: 0.837397575378418\n",
            "Val loss: 0.5130924761295319, Val f1: 0.8175137639045715\n",
            "Val loss: 0.5064393281936646, Val f1: 0.8077194690704346\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.43553834236585176\n",
            "Train loss: 0.4254907260212717\n",
            "Train loss: 0.417007787387912\n",
            "Train loss: 0.4123650847366021\n",
            "Train loss: 0.4078839690841023\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.37628944027118194, Val f1: 0.8609787225723267\n",
            "Val loss: 0.36716883582404897, Val f1: 0.8527188301086426\n",
            "Val loss: 0.3582755860661258, Val f1: 0.8538034558296204\n",
            "Val loss: 0.3517974442655935, Val f1: 0.856076180934906\n",
            "Val loss: 0.346371079958863, Val f1: 0.8582573533058167\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.6252497831980387, Val f1: 1.0403616428375244\n",
            "Val loss: 0.5483785441943577, Val f1: 0.8879547715187073\n",
            "Val loss: 0.5217799761078574, Val f1: 0.8472750782966614\n",
            "Val loss: 0.5141767422358196, Val f1: 0.8258144855499268\n",
            "Val loss: 0.5075135058478305, Val f1: 0.81560218334198\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.39275005841866517\n",
            "Train loss: 0.3845711912535414\n",
            "Train loss: 0.3770926068810856\n",
            "Train loss: 0.3732327670796112\n",
            "Train loss: 0.3699345372280284\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.33806431675568605, Val f1: 0.8810985684394836\n",
            "Val loss: 0.3287436347219008, Val f1: 0.8721761703491211\n",
            "Val loss: 0.3201602198997465, Val f1: 0.8723416328430176\n",
            "Val loss: 0.31318831181376233, Val f1: 0.8744305968284607\n",
            "Val loss: 0.30731183395313855, Val f1: 0.8767482042312622\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.6400351425011953, Val f1: 1.038560152053833\n",
            "Val loss: 0.5626627334526607, Val f1: 0.8903270959854126\n",
            "Val loss: 0.5356368720531464, Val f1: 0.8493474125862122\n",
            "Val loss: 0.5291542629400889, Val f1: 0.826845645904541\n",
            "Val loss: 0.5220160186290741, Val f1: 0.8157927393913269\n",
            "\n",
            "starting Epoch 5\n",
            "Training...\n",
            "Train loss: 0.3603460918634366\n",
            "Train loss: 0.35397786882859245\n",
            "Train loss: 0.3465957586504832\n",
            "Train loss: 0.343649982094015\n",
            "Train loss: 0.34065420656048473\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.3064670669726836, Val f1: 0.8956072926521301\n",
            "Val loss: 0.29786946351015114, Val f1: 0.8870605230331421\n",
            "Val loss: 0.28966734639736785, Val f1: 0.8874649405479431\n",
            "Val loss: 0.2827851245613218, Val f1: 0.8895564675331116\n",
            "Val loss: 0.27690980744421784, Val f1: 0.8917564153671265\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.6555992662906647, Val f1: 1.0410935878753662\n",
            "Val loss: 0.577875771692821, Val f1: 0.8858476877212524\n",
            "Val loss: 0.5487474392760884, Val f1: 0.8459004759788513\n",
            "Val loss: 0.5430578847726186, Val f1: 0.8241597414016724\n",
            "Val loss: 0.5361993328521126, Val f1: 0.8140600919723511\n",
            "\n",
            "starting Epoch 6\n",
            "Training...\n",
            "Train loss: 0.3299824419694069\n",
            "Train loss: 0.3262799487838262\n",
            "Train loss: 0.32191966436490294\n",
            "Train loss: 0.319063053565955\n",
            "Train loss: 0.3158393131578388\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.2818407863378525, Val f1: 0.9069272875785828\n",
            "Val loss: 0.2729893345621568, Val f1: 0.8992734551429749\n",
            "Val loss: 0.26554666672434124, Val f1: 0.8989962935447693\n",
            "Val loss: 0.2587798278099336, Val f1: 0.9011257886886597\n",
            "Val loss: 0.2528009018556556, Val f1: 0.9033953547477722\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.6794128020604452, Val f1: 1.0332057476043701\n",
            "Val loss: 0.5987235648291451, Val f1: 0.8830592036247253\n",
            "Val loss: 0.5700310116464441, Val f1: 0.8427731394767761\n",
            "Val loss: 0.5645791033903758, Val f1: 0.8204434514045715\n",
            "Val loss: 0.5571993761941006, Val f1: 0.8102433085441589\n",
            "\n",
            "starting Epoch 7\n",
            "Training...\n",
            "Train loss: 0.3040908766098512\n",
            "Train loss: 0.29920189139209213\n",
            "Train loss: 0.2969997425039275\n",
            "Train loss: 0.2963377572830368\n",
            "Train loss: 0.2930738001912083\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.26411927166657573, Val f1: 0.9193139672279358\n",
            "Val loss: 0.2571118249546123, Val f1: 0.9100823998451233\n",
            "Val loss: 0.24996961678276544, Val f1: 0.9095433354377747\n",
            "Val loss: 0.2442251818734895, Val f1: 0.9107372760772705\n",
            "Val loss: 0.23855628395200373, Val f1: 0.9127743244171143\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.7153759102026621, Val f1: 1.0474390983581543\n",
            "Val loss: 0.6272650148187365, Val f1: 0.8947863578796387\n",
            "Val loss: 0.6004325341094624, Val f1: 0.8531582951545715\n",
            "Val loss: 0.5950407524903615, Val f1: 0.8319838643074036\n",
            "Val loss: 0.5858079022482822, Val f1: 0.8222228288650513\n",
            "\n",
            "starting Epoch 8\n",
            "Training...\n",
            "Train loss: 0.2851080898291025\n",
            "Train loss: 0.2822100678576699\n",
            "Train loss: 0.2771443737154247\n",
            "Train loss: 0.2766824697361052\n",
            "Train loss: 0.27367712871812694\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.245320153542054, Val f1: 0.9285888671875\n",
            "Val loss: 0.23791660274131388, Val f1: 0.9192253947257996\n",
            "Val loss: 0.23133918405080042, Val f1: 0.918276309967041\n",
            "Val loss: 0.22602180686761747, Val f1: 0.9193488955497742\n",
            "Val loss: 0.22055214314005483, Val f1: 0.9216116666793823\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.7392502824465433, Val f1: 1.0503244400024414\n",
            "Val loss: 0.6499573418072292, Val f1: 0.8929607272148132\n",
            "Val loss: 0.6228386109525507, Val f1: 0.8506585359573364\n",
            "Val loss: 0.6183978954950968, Val f1: 0.8289659023284912\n",
            "Val loss: 0.6086239846129167, Val f1: 0.8193641901016235\n",
            "\n",
            "starting Epoch 9\n",
            "Training...\n",
            "Train loss: 0.2656599821952673\n",
            "Train loss: 0.26096942473815965\n",
            "Train loss: 0.2556080684191039\n",
            "Train loss: 0.255449929139899\n",
            "Train loss: 0.2525444650021031\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.2221746910841037, Val f1: 0.9395127296447754\n",
            "Val loss: 0.2157417954523352, Val f1: 0.9293534755706787\n",
            "Val loss: 0.2094983384639275, Val f1: 0.9277302026748657\n",
            "Val loss: 0.20395745355753028, Val f1: 0.9286584258079529\n",
            "Val loss: 0.19859757046004636, Val f1: 0.9304220080375671\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.7929133772850037, Val f1: 1.0438969135284424\n",
            "Val loss: 0.6974471381732396, Val f1: 0.8903834819793701\n",
            "Val loss: 0.6691756573590365, Val f1: 0.8473767042160034\n",
            "Val loss: 0.6627470652262369, Val f1: 0.8275158405303955\n",
            "Val loss: 0.6566429451892251, Val f1: 0.8172189593315125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiMwAZbIJ3-H"
      },
      "source": [
        "С дропаутами стало действительно лучший лосс на тесте действительно был немного получше, но буквально на одну сотую. Зато скетка медленнее переобучается. Если увличить размер эмбеддинга до 300, становится еще немного лучше. Плюс я добавила еще одну свертку после конкатенации.\n",
        "Можно сравнить лучший лосс на тесте у модели со всеми модификациями и лучший лосс на тесте у CNN_pretrained. \n",
        "\n",
        "CNN_pretrained\n",
        "\n",
        "    Val loss: 0.6978476444880167, Val f1: 0.97179114818573\n",
        "    Val loss: 0.5950382266725812, Val f1: 0.8298261165618896\n",
        "    Val loss: 0.5670244639570062, Val f1: 0.7905034422874451\n",
        "    Val loss: 0.5577905774116516, Val f1: 0.7714343667030334\n",
        "    Val loss: 0.5465866139060572, Val f1: 0.7661719918251038\n",
        "\n",
        "CNN_pretrained_mod \n",
        "\n",
        "    Val loss: 0.6252497831980387, Val f1: 1.0403616428375244\n",
        "    Val loss: 0.5483785441943577, Val f1: 0.8879547715187073\n",
        "    Val loss: 0.5217799761078574, Val f1: 0.8472750782966614\n",
        "    Val loss: 0.5141767422358196, Val f1: 0.8258144855499268\n",
        "    Val loss: 0.5075135058478305, Val f1: 0.81560218334198"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZ-ONpO2ndHl"
      },
      "source": [
        "### Готовим данные 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mjy8ZVteniBs",
        "outputId": "61aa2b4e-2df4-47f6-9476-006dc18b05b6"
      },
      "source": [
        "vocab = Counter()\n",
        "for text in tweets_data['text']:\n",
        "    vocab.update(list(text))\n",
        "print('всего уникальных символов:', len(vocab))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "всего уникальных символов: 456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mr_LygrnojgE",
        "outputId": "0e01de4f-82b8-47f1-bb7d-87b0491ceea0"
      },
      "source": [
        "filtered_vocab = set()\n",
        "\n",
        "for symbol in vocab:\n",
        "    if vocab[symbol] > 5:\n",
        "        filtered_vocab.add(symbol)\n",
        "print('уникальных символов, втретившихся больше 5 раз:', len(filtered_vocab))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "уникальных символов, втретившихся больше 5 раз: 234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9FbRas9ooXl",
        "outputId": "b0a48d48-1e09-4e09-cf20-7d50cd5624be"
      },
      "source": [
        "#создаем словарь с индексами symbol2id, для спецсимвола паддинга дефолтный индекс - 0\n",
        "symbol2id = {'PAD':0}\n",
        "\n",
        "for symbol in filtered_vocab:\n",
        "    symbol2id[symbol] = len(symbol2id)\n",
        "\n",
        "#обратный словарь для того, чтобы раскодировать последовательность\n",
        "id2symbol = {i:symbol for symbol, i in symbol2id.items()}\n",
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "DEVICE"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dt7nK4Dnox7U"
      },
      "source": [
        "class TweetsDataset2(Dataset):\n",
        "\n",
        "    def __init__(self, dataset, symbol2id, word2id, DEVICE):\n",
        "        self.dataset = dataset['text'].values\n",
        "        self.symbol2id = symbol2id\n",
        "        self.word2id = word2id\n",
        "        self.length = dataset.shape[0]\n",
        "        self.target = dataset['tone'].values\n",
        "        self.device = DEVICE\n",
        "\n",
        "    def __len__(self): #это обязательный метод, он должен уметь считать длину датасета\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index): #еще один обязательный метод. По индексу возвращает элемент выборки\n",
        "        text = self.dataset[index]\n",
        "        tokens = self.preprocess(text) # токенизируем\n",
        "        ids = torch.LongTensor([self.word2id[token] for token in tokens if token in self.word2id])\n",
        "        symbol_ids = torch.LongTensor([self.symbol2id[symbol] for symbol in text if symbol in self.symbol2id])\n",
        "        y = [self.target[index]]\n",
        "        return ids, symbol_ids, y\n",
        "    \n",
        "    def preprocess(self, text):\n",
        "        tokens = text.lower().split()\n",
        "        tokens = [token.strip(punctuation) for token in tokens]\n",
        "        tokens = [token for token in tokens if token]\n",
        "        return tokens\n",
        "\n",
        "    def collate_fn(self, batch): #этот метод можно реализовывать и отдельно,\n",
        "    # он понадобится для DataLoader во время итерации по батчам\n",
        "      ids, symbol_ids, y = list(zip(*batch))\n",
        "      padded_symbol_ids = pad_sequence(symbol_ids, batch_first=True).to(self.device)\n",
        "      padded_ids = pad_sequence(ids, batch_first=True).to(self.device)\n",
        "      #мы хотим применять BCELoss, он будет брать на вход predicted размера batch_size x 1 (так как для каждого семпла модель будет отдавать одно число), target размера batch_size x 1\n",
        "      y = torch.Tensor(y).to(self.device) # tuple ([1], [0], [1])  -> Tensor [[1.], [0.], [1.]] \n",
        "      return (padded_ids, padded_symbol_ids), y"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nfx26i6rV87"
      },
      "source": [
        "train_dataset = TweetsDataset2(train_sentences, symbol2id, word2id, DEVICE)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn, sampler=train_sampler, batch_size=1024)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuEmyjCkz29a"
      },
      "source": [
        "val_dataset = TweetsDataset2(val_sentences, symbol2id, word2id, DEVICE)\n",
        "val_sampler = RandomSampler(val_dataset)\n",
        "val_iterator = DataLoader(val_dataset, collate_fn = val_dataset.collate_fn, sampler=val_sampler, batch_size=1024)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjDy9kUjLnTO"
      },
      "source": [
        "### Архитектура 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBk8vhgoLt80"
      },
      "source": [
        "class CNN2(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size_word, vocab_size_symbol, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding_word = nn.Embedding(vocab_size_word, embedding_dim)\n",
        "        self.hidden_word = nn.Linear(in_features=embedding_dim, out_features=100)\n",
        "        self.embedding_symbol = nn.Embedding(vocab_size_symbol, embedding_dim)\n",
        "        self.bigrams_symbol = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=2, padding='same')\n",
        "        self.trigrams_symbol = nn.Conv1d(in_channels=embedding_dim, out_channels=80, kernel_size=3, padding='same')\n",
        "        self.hidden = nn.Linear(in_features=280, out_features=1)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, text):\n",
        "        embedded_word = self.embedding_word(text[0])\n",
        "        embedded_sent = torch.mean(embedded_word, dim=1)\n",
        "        embedded_sent = self.hidden_word(embedded_sent)\n",
        "\n",
        "        embedded_symbol = self.embedding_symbol(text[1])\n",
        "        embedded_symbol = embedded_symbol.transpose(1,2)\n",
        "        feature_map_bigrams = self.bigrams_symbol(embedded_symbol)\n",
        "        feature_map_trigrams = self.trigrams_symbol(embedded_symbol)\n",
        "        concat = torch.cat((feature_map_bigrams, feature_map_trigrams), 1)\n",
        "        pooling = concat.max(2)[0]\n",
        "\n",
        "        concat = torch.cat((embedded_sent, pooling), 1)\n",
        "        logits = self.hidden(concat) \n",
        "        logits = self.out(logits)      \n",
        "        return logits"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FS72XNoC2u7P"
      },
      "source": [
        "model = CNN2(len(word2id), len(symbol2id), 100)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.BCELoss() # Binary Cross Entropy\n",
        "\n",
        "# веса модели и значения лосса храним там же, где и все остальные тензоры\n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhZ84yTG3Kal",
        "outputId": "3ec63f63-219a-4b40-8862-df8f7c671ff8"
      },
      "source": [
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(10):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = train(model, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = evaluate(model, train_iterator, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/conv.py:298: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at  ../aten/src/ATen/native/Convolution.cpp:647.)\n",
            "  self.padding, self.dilation, self.groups)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train loss: 0.3490513395040463\n",
            "Train loss: 0.18837691278821683\n",
            "Train loss: 0.12850420717776073\n",
            "Train loss: 0.09754185938077199\n",
            "Train loss: 0.07857494403891284\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.0023157291848642323, Val f1: 1.0255930423736572\n",
            "Val loss: 0.0024024410205716385, Val f1: 1.0125236511230469\n",
            "Val loss: 0.002356087035663864, Val f1: 1.0082893371582031\n",
            "Val loss: 0.002359549527396243, Val f1: 1.0061793327331543\n",
            "Val loss: 0.0023539833245967427, Val f1: 1.0049028396606445\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0035546041714648404, Val f1: 1.332688570022583\n",
            "Val loss: 0.003054564253294042, Val f1: 1.1423078775405884\n",
            "Val loss: 0.00288235245865177, Val f1: 1.0905596017837524\n",
            "Val loss: 0.0030178340928008157, Val f1: 1.0662821531295776\n",
            "Val loss: 0.0028176211604946538, Val f1: 1.0523279905319214\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 0.0020871655149862934\n",
            "Train loss: 0.0018177537086914895\n",
            "Train loss: 0.001598788721745192\n",
            "Train loss: 0.0014294062689116588\n",
            "Train loss: 0.0012837122537426174\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.0006982205123700297, Val f1: 1.0256171226501465\n",
            "Val loss: 0.0007145643032368132, Val f1: 1.0126097202301025\n",
            "Val loss: 0.0006907922252445255, Val f1: 1.0083712339401245\n",
            "Val loss: 0.000677225715737005, Val f1: 1.0062651634216309\n",
            "Val loss: 0.0006959086838915438, Val f1: 1.004996418952942\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0015688186783033113, Val f1: 1.3330204486846924\n",
            "Val loss: 0.0013534902495199016, Val f1: 1.1425777673721313\n",
            "Val loss: 0.0011303247989748013, Val f1: 1.0907312631607056\n",
            "Val loss: 0.0010614190056609611, Val f1: 1.0664713382720947\n",
            "Val loss: 0.0010604920772541511, Val f1: 1.0524275302886963\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.0006583255949991349\n",
            "Train loss: 0.0006055539238850197\n",
            "Train loss: 0.0005942611600968893\n",
            "Train loss: 0.0005533669788958563\n",
            "Train loss: 0.0005119271126245827\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.0003439376361291999, Val f1: 1.0256410837173462\n",
            "Val loss: 0.0003661086393779592, Val f1: 1.0126457214355469\n",
            "Val loss: 0.00035413862387023617, Val f1: 1.0083950757980347\n",
            "Val loss: 0.000358455106777098, Val f1: 1.0062769651412964\n",
            "Val loss: 0.0003546327328392487, Val f1: 1.005015254020691\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0004286240340055277, Val f1: 1.3333333730697632\n",
            "Val loss: 0.00040455685978356214, Val f1: 1.1428571939468384\n",
            "Val loss: 0.000447309292874045, Val f1: 1.0909091234207153\n",
            "Val loss: 0.00041840020470165957, Val f1: 1.0666667222976685\n",
            "Val loss: 0.0005026516760393095, Val f1: 1.0525844097137451\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.0003740527608897537\n",
            "Train loss: 0.0003237570078764995\n",
            "Train loss: 0.0003080259177157804\n",
            "Train loss: 0.00028534158252497383\n",
            "Train loss: 0.00028660730477077326\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.0002485892760794228, Val f1: 1.0256410837173462\n",
            "Val loss: 0.0002490907164785681, Val f1: 1.0126458406448364\n",
            "Val loss: 0.0002334274655108207, Val f1: 1.0083951950073242\n",
            "Val loss: 0.00022606636409804447, Val f1: 1.006283164024353\n",
            "Val loss: 0.00022940680384782175, Val f1: 1.005015254020691\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.0002725536663395663, Val f1: 1.3333333730697632\n",
            "Val loss: 0.0003460226684442854, Val f1: 1.1428571939468384\n",
            "Val loss: 0.0003130033345025202, Val f1: 1.0909091234207153\n",
            "Val loss: 0.000623325333193255, Val f1: 1.0665390491485596\n",
            "Val loss: 0.0005805962389690409, Val f1: 1.0524808168411255\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.0002202846334488967\n",
            "Train loss: 0.0002069664777174026\n",
            "Train loss: 0.00020749322353948315\n",
            "Train loss: 0.00019339133082755766\n",
            "Train loss: 0.0001871880558610428\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.00012611924728098063, Val f1: 1.0256410837173462\n",
            "Val loss: 0.00013162234001034057, Val f1: 1.0126582384109497\n",
            "Val loss: 0.0001378549284464582, Val f1: 1.0084034204483032\n",
            "Val loss: 0.00013795065870742847, Val f1: 1.0062892436981201\n",
            "Val loss: 0.0001460053530326008, Val f1: 1.005020022392273\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.00025328568153781816, Val f1: 1.3333333730697632\n",
            "Val loss: 0.00026041208392208707, Val f1: 1.1428571939468384\n",
            "Val loss: 0.00032803853404898706, Val f1: 1.0908197164535522\n",
            "Val loss: 0.000277998523961287, Val f1: 1.0666011571884155\n",
            "Val loss: 0.0002483910249889289, Val f1: 1.0525797605514526\n",
            "\n",
            "starting Epoch 5\n",
            "Training...\n",
            "Train loss: 0.00012788204222511596\n",
            "Train loss: 0.0001280584590490377\n",
            "Train loss: 0.00012283243803026648\n",
            "Train loss: 0.0001267101101316629\n",
            "Train loss: 0.00013094648796200387\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.00012357225858021336, Val f1: 1.02561616897583\n",
            "Val loss: 0.00011445864086603757, Val f1: 1.012645959854126\n",
            "Val loss: 0.00011602314547982904, Val f1: 1.0083953142166138\n",
            "Val loss: 0.00011091576766518513, Val f1: 1.006283164024353\n",
            "Val loss: 0.00011097459412041242, Val f1: 1.005020260810852\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.001024515018798411, Val f1: 1.333031177520752\n",
            "Val loss: 0.0008900536517363175, Val f1: 1.142586588859558\n",
            "Val loss: 0.0006024317534121855, Val f1: 1.090736985206604\n",
            "Val loss: 0.0005276218730917511, Val f1: 1.0664767026901245\n",
            "Val loss: 0.00045922800544710636, Val f1: 1.0524816513061523\n",
            "\n",
            "starting Epoch 6\n",
            "Training...\n",
            "Train loss: 0.00010282900210511751\n",
            "Train loss: 9.917710628507871e-05\n",
            "Train loss: 0.00010907710072517927\n",
            "Train loss: 9.891397738948233e-05\n",
            "Train loss: 9.730227055060549e-05\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 6.916531609208621e-05, Val f1: 1.0256410837173462\n",
            "Val loss: 8.043722825473369e-05, Val f1: 1.0126582384109497\n",
            "Val loss: 8.751209636958425e-05, Val f1: 1.008394718170166\n",
            "Val loss: 8.423569585755322e-05, Val f1: 1.0062828063964844\n",
            "Val loss: 7.955912774400646e-05, Val f1: 1.0050199031829834\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 8.454850467387587e-05, Val f1: 1.3333333730697632\n",
            "Val loss: 0.000556987793450909, Val f1: 1.1425776481628418\n",
            "Val loss: 0.000599256159727123, Val f1: 1.0906412601470947\n",
            "Val loss: 0.00047284901035406316, Val f1: 1.0664703845977783\n",
            "Val loss: 0.00039264095309925705, Val f1: 1.0524766445159912\n",
            "\n",
            "starting Epoch 7\n",
            "Training...\n",
            "Train loss: 8.544841773236373e-05\n",
            "Train loss: 7.871860571303611e-05\n",
            "Train loss: 8.559692596005151e-05\n",
            "Train loss: 7.772285312486654e-05\n",
            "Train loss: 7.369929989268237e-05\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 6.475496224876947e-05, Val f1: 1.0256410837173462\n",
            "Val loss: 6.418170027867331e-05, Val f1: 1.0126582384109497\n",
            "Val loss: 7.173344453781091e-05, Val f1: 1.0083951950073242\n",
            "Val loss: 6.939791496991365e-05, Val f1: 1.006283164024353\n",
            "Val loss: 6.347553349385535e-05, Val f1: 1.005020260810852\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 8.919238462112844e-05, Val f1: 1.3333333730697632\n",
            "Val loss: 7.335338991002313e-05, Val f1: 1.1428571939468384\n",
            "Val loss: 0.000154513119592924, Val f1: 1.0908228158950806\n",
            "Val loss: 0.00026005440595326944, Val f1: 1.066537618637085\n",
            "Val loss: 0.0003477890715753586, Val f1: 1.0524779558181763\n",
            "\n",
            "starting Epoch 8\n",
            "Training...\n",
            "Train loss: 5.816991133309411e-05\n",
            "Train loss: 6.527589636179176e-05\n",
            "Train loss: 6.394859292031317e-05\n",
            "Train loss: 5.9652944755668714e-05\n",
            "Train loss: 5.752426479841745e-05\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 5.724166182447404e-05, Val f1: 1.0256410837173462\n",
            "Val loss: 5.196690515823562e-05, Val f1: 1.0126582384109497\n",
            "Val loss: 5.001245991487307e-05, Val f1: 1.0084034204483032\n",
            "Val loss: 4.79537589030169e-05, Val f1: 1.0062892436981201\n",
            "Val loss: 4.816329523301057e-05, Val f1: 1.0050251483917236\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 6.869251713699971e-05, Val f1: 1.3333333730697632\n",
            "Val loss: 0.00032988937787844667, Val f1: 1.142727255821228\n",
            "Val loss: 0.0002687161408671686, Val f1: 1.090826392173767\n",
            "Val loss: 0.0003894063981230526, Val f1: 1.0665427446365356\n",
            "Val loss: 0.0003587551469344793, Val f1: 1.0524852275848389\n",
            "\n",
            "starting Epoch 9\n",
            "Training...\n",
            "Train loss: 6.040234564772198e-05\n",
            "Train loss: 5.145071949461649e-05\n",
            "Train loss: 4.882770323291413e-05\n",
            "Train loss: 4.70047805365596e-05\n",
            "Train loss: 4.489548413787007e-05\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 3.3502940870022983e-05, Val f1: 1.0256410837173462\n",
            "Val loss: 3.5361259874719775e-05, Val f1: 1.0126582384109497\n",
            "Val loss: 3.589733279579928e-05, Val f1: 1.0084034204483032\n",
            "Val loss: 3.5992164758115324e-05, Val f1: 1.0062892436981201\n",
            "Val loss: 3.82898517236571e-05, Val f1: 1.0050251483917236\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 3.057207019689182e-05, Val f1: 1.3333333730697632\n",
            "Val loss: 5.978561083403682e-05, Val f1: 1.1428571939468384\n",
            "Val loss: 9.341573472325267e-05, Val f1: 1.0909091234207153\n",
            "Val loss: 8.142352817230858e-05, Val f1: 1.0666667222976685\n",
            "Val loss: 0.00034474919390815656, Val f1: 1.0524795055389404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQmXkq6BcNge"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}